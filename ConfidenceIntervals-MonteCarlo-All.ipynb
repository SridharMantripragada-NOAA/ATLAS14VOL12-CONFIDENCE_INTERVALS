{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07f1e6f-4949-4577-b085-5e1c6a396cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from lmoments3 import distr\n",
    "\n",
    "from modules.api import load_config, extract_regions, extract_coloc \n",
    "from modules.api import corrmatrix, samlmomgev, Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d94449-ec76-43bd-80c7-59a76e02fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from config file\n",
    "config = load_config(Path('config.json'))\n",
    "dir_root = config['dir_root']\n",
    "region_file = config['region_file']\n",
    "prism_file = config['prism_file']\n",
    "meta_file = config['meta_file']\n",
    "coloc_file = config['coloc_file']\n",
    "ARI = np.array(config['ARI']) # Average Recurrence Interval\n",
    "aep = 1 - 1/ARI  # Annual Exceedance Probability\n",
    "base_durations = config['base_durations']\n",
    "prism_durations = config['prism_durations']\n",
    "nsim = config['nsim'] # Number of Monte Carlo Simulations\n",
    "boundprob = config['boundprob'] # 90% CI\n",
    "nmom = config['nmom'] # number of L-moments\n",
    "\n",
    "distributions = [distr.gev, distr.gno, distr.glo, distr.pe3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4128e2bd-6b50-4544-ac14-b75af3439ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metdata\n",
    "df_meta = pd.read_parquet(Path(meta_file), engine='pyarrow')\n",
    "# Read PRISM MAM\n",
    "prism_mam_df = pd.read_csv(Path(prism_file), sep = '\\s+')\n",
    "prism_mam_df = prism_mam_df.rename(columns={'Site_ID': 'HDSC'}).set_index('HDSC', drop=True)\n",
    "# Read regionalization information\n",
    "sids, regions = extract_regions(Path(region_file))\n",
    "coloc = extract_coloc(coloc_file)\n",
    "# exclude station ids that are in buffer region.\n",
    "#sids = coloc[(coloc['HDSC_ID'].isin(sids)) & (coloc['ST'].isin(['ID', 'WY', 'MT']))]['HDSC_ID'].unique()\n",
    "df_meta1 = pd.concat([df_meta[df_meta['HDSC'] == sid] for sid in sids])\n",
    "lat = df_meta1['LAT'].values\n",
    "lon = df_meta1['LON'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "979b515c-9a1a-4a96-b113-f8a203144389",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant = np.full((len(base_durations), len(sids), len(ARI)), np.nan)\n",
    "lbound = np.full((len(base_durations), len(sids), len(ARI)), np.nan)\n",
    "ubound = np.full((len(base_durations), len(sids), len(ARI)), np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db5cb23-5240-459a-a3d6-1127154610c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Duration: 60m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stations: 100%|██████████| 1521/1521 [29:08<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Duration: 06h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stations: 100%|██████████| 1521/1521 [28:59<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Duration: 24h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stations: 100%|██████████| 1521/1521 [48:40<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Duration: 04d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stations: 100%|██████████| 1521/1521 [48:27<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Duration: 10d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stations: 100%|██████████| 1521/1521 [48:25<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Duration: 30d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stations: 100%|██████████| 1521/1521 [48:28<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Duration: 60d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing stations: 100%|██████████| 1521/1521 [48:27<00:00,  1.91s/it]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "for index1, (base, prism_duration) in enumerate(zip(base_durations, prism_durations)):\n",
    "    print(f'Base Duration: {base}')\n",
    "    # Read the AMS for specifc base duration\n",
    "    ams_df = pd.read_parquet(Path(dir_root, f'df_ams_{base}.parquet.snappy'), engine='pyarrow')\n",
    "    ams_df = ams_df.join(df_meta['HDSC'])\n",
    "    ams_df = ams_df.set_index('HDSC', drop=True)\n",
    "    for index2, sid in tqdm(enumerate(sids), total=len(sids), desc=\"Processing stations\"):\n",
    "        sid_region = [sid] + regions[sid]\n",
    "        # Extract AMS for all stations in a region\n",
    "        ams = ams_df[ams_df.index.isin(sid_region)].copy()\n",
    "        # Get PRISM MAM\n",
    "        mam_prism = prism_mam_df[prism_mam_df.index.isin(sid_region)][prism_duration].copy()\n",
    "        # Calculate the at-site L-moments and GEV paameters\n",
    "        lmoms, paras = samlmomgev(ams, mam_prism)\n",
    "\n",
    "        # Get station MAM\n",
    "        smam = mam_prism.loc[sid]\n",
    "        if np.isnan(smam):\n",
    "            if sid not in lmoms.index:\n",
    "                #print(f'No MAM for {sid} at {base} duration. Proceed to the next station.')\n",
    "                continue\n",
    "            else:\n",
    "                smam = lmoms.loc[sid]['mean']\n",
    "\n",
    "        # Calculate inter-site correleation coefficient\n",
    "        rho, sd_rho = corrmatrix(ams)\n",
    "        # Run Monte Carlo Simulation\n",
    "        sim = Simulation(sid, base, rho, sd_rho, ARI, lmoms, smam, nsim, distributions, boundprob)\n",
    "        sim.run_simulations()\n",
    "        quant[index1, index2, :], lbound[index1, index2, :], ubound[index1, index2, :] = sim.bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52da6d53-419c-4c14-bc6d-d102792c9673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save data to netcdf file\n"
     ]
    }
   ],
   "source": [
    "# Save data to netcdf file\n",
    "print('Save data to netcdf file')\n",
    "\n",
    "quant_da = xr.DataArray(quant, dims=['duration', 'hdsc', 'ari'],\n",
    "                         coords={'duration': base_durations,\n",
    "                                 'hdsc': sids, 'ari': ARI})\n",
    "\n",
    "lbound_da = xr.DataArray(lbound, dims=['duration', 'hdsc', 'ari'], \n",
    "                         coords={'duration': base_durations,\n",
    "                                 'hdsc': sids, 'ari': ARI})\n",
    "\n",
    "ubound_da = xr.DataArray(ubound, dims=['duration', 'hdsc', 'ari'], \n",
    "                           coords={'duration': base_durations,\n",
    "                                   'hdsc': sids, 'ari': ARI})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0cb6476-3782-45e0-adb7-f7c55663dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset({\n",
    "    'quantile': quant_da,\n",
    "    'lbound': lbound_da,\n",
    "    'ubound': ubound_da,\n",
    "})\n",
    "\n",
    "metadata = {\n",
    "    'lbound': {\n",
    "        'long_name': 'Lower Confidence Interval',\n",
    "        'description': 'Lower confidence interval of the GEV estimate',\n",
    "        'units': 'inches'\n",
    "    },\n",
    "    'ubound': {\n",
    "        'long_name': 'Upper Confidence Interval',\n",
    "        'description': 'Upper confidence interval of the GEV estimate',\n",
    "        'units': 'inches'\n",
    "    },\n",
    "    'quantile': {\n",
    "        'long_name': 'GEV Estimate',\n",
    "        'description': 'Regional GEV Estimate of the annual maximum precipitation',\n",
    "        'units': 'inches'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "for var_name, attrs in metadata.items():\n",
    "    ds[var_name].attrs.update(attrs)\n",
    "\n",
    "ds.coords['lat'] = ('hdsc', lat)\n",
    "ds.coords['lon'] = ('hdsc', lon)\n",
    "\n",
    "# ds.to_netcdf('regional_stats_montecarlo_cibounds.nc')\n",
    "ds.to_netcdf('MonteCarlo_CIbounds.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f29fa6-1ba5-4eee-b87a-3d321273b63e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cibounds",
   "language": "python",
   "name": "cibounds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
